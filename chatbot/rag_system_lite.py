"""
ê²½ëŸ‰ RAG ì‹œìŠ¤í…œ (TF-IDF ê¸°ë°˜)
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (~10-50MB)
- sentence-transformers ëŒ€ì‹  TF-IDF ì‚¬ìš©
- 512MB ì œí•œ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
"""

import os
import pickle
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Tuple
from pdf_processor import PDFProcessor


class RAGSystemLite:
    """ê²½ëŸ‰ RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤ (TF-IDF ê¸°ë°˜)"""
    
    def __init__(self):
        """
        TF-IDF ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”
        """
        print("ğŸ”„ ê²½ëŸ‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ (TF-IDF)...")
        
        # TF-IDF ë²¡í„°ë¼ì´ì € (í•œêµ­ì–´ ì§€ì›)
        self.vectorizer = TfidfVectorizer(
            max_features=5000,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            ngram_range=(1, 2),  # 1-gram, 2-gram ì‚¬ìš©
            min_df=1,
            max_df=0.95,
            sublinear_tf=True  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§
        )
        
        self.chunks = []
        self.tfidf_matrix = None
        
        print("âœ… ê²½ëŸ‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def build_index(self, chunks: List[Dict[str, any]]):
        """
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° TF-IDF ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            chunks: PDF ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not chunks:
            print("âš ï¸ ì²­í¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ”„ {len(chunks)}ê°œ ì²­í¬ ì¸ë±ì‹± ì¤‘ (TF-IDF)...")
        
        # ì²­í¬ ì €ì¥
        self.chunks = chunks
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [chunk['text'] for chunk in chunks]
        
        # TF-IDF í–‰ë ¬ ìƒì„±
        self.tfidf_matrix = self.vectorizer.fit_transform(texts)
        
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ ë¬¸ì„œ, {self.tfidf_matrix.shape[1]}ê°œ íŠ¹ì„±")
    
    def search(self, query: str, top_k: int = 3) -> List[Tuple[Dict, float]]:
        """
        ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            (ì²­í¬, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if self.tfidf_matrix is None or len(self.chunks) == 0:
            print("âš ï¸ ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return []
        
        # ì§ˆë¬¸ ë²¡í„°í™”
        query_vector = self.vectorizer.transform([query])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        # ê²°ê³¼ ì •ë¦¬
        results = []
        for idx in top_indices:
            if idx < len(self.chunks):
                similarity = float(similarities[idx])
                results.append((self.chunks[idx], similarity))
        
        return results
    
    def generate_answer(self, query: str, top_k: int = 3, similarity_threshold: float = 0.1) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)
            
        Returns:
            ë‹µë³€ ë”•ì…”ë„ˆë¦¬
        """
        # ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
        results = self.search(query, top_k)
        
        if not results:
            return {
                'answer': None,
                'sources': [],
                'confidence': 'none'
            }
        
        # ê°€ì¥ ìœ ì‚¬í•œ ê²°ê³¼ í™•ì¸
        best_chunk, best_similarity = results[0]
        
        # ì„ê³„ê°’ ì²´í¬
        if best_similarity < similarity_threshold:
            return {
                'answer': None,
                'sources': [],
                'confidence': 'low',
                'similarity': best_similarity
            }
        
        # ë‹µë³€ ìƒì„± (ìƒìœ„ ê²°ê³¼ ì¡°í•©)
        answer_parts = []
        sources = []
        
        for chunk, similarity in results[:2]:  # ìƒìœ„ 2ê°œë§Œ ì‚¬ìš©
            if similarity >= similarity_threshold:
                answer_parts.append(chunk['text'])
                sources.append({
                    'page': chunk['metadata'].get('page_number', '?'),
                    'source': chunk['metadata'].get('source', ''),
                    'similarity': similarity
                })
        
        if not answer_parts:
            return {
                'answer': None,
                'sources': [],
                'confidence': 'low'
            }
        
        # ë‹µë³€ í¬ë§·íŒ…
        answer = "\n\n".join(answer_parts)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = 'high' if best_similarity > 0.3 else 'medium'
        
        return {
            'answer': answer,
            'sources': sources,
            'confidence': confidence,
            'similarity': best_similarity
        }
    
    def save_index(self, index_dir: str):
        """
        ì¸ë±ìŠ¤ì™€ ì²­í¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            index_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        """
        os.makedirs(index_dir, exist_ok=True)
        
        # TF-IDF ë²¡í„°ë¼ì´ì € ì €ì¥
        vectorizer_path = os.path.join(index_dir, 'vectorizer.pkl')
        with open(vectorizer_path, 'wb') as f:
            pickle.dump(self.vectorizer, f)
        
        # TF-IDF í–‰ë ¬ ì €ì¥
        matrix_path = os.path.join(index_dir, 'tfidf_matrix.pkl')
        with open(matrix_path, 'wb') as f:
            pickle.dump(self.tfidf_matrix, f)
        
        # ì²­í¬ ì €ì¥
        chunks_path = os.path.join(index_dir, 'chunks.pkl')
        with open(chunks_path, 'wb') as f:
            pickle.dump(self.chunks, f)
        
        print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_dir}")
    
    def load_index(self, index_dir: str) -> bool:
        """
        ì €ì¥ëœ ì¸ë±ìŠ¤ì™€ ì²­í¬ ë¡œë“œ
        
        Args:
            index_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # TF-IDF ë²¡í„°ë¼ì´ì € ë¡œë“œ
            vectorizer_path = os.path.join(index_dir, 'vectorizer.pkl')
            with open(vectorizer_path, 'rb') as f:
                self.vectorizer = pickle.load(f)
            
            # TF-IDF í–‰ë ¬ ë¡œë“œ
            matrix_path = os.path.join(index_dir, 'tfidf_matrix.pkl')
            with open(matrix_path, 'rb') as f:
                self.tfidf_matrix = pickle.load(f)
            
            # ì²­í¬ ë¡œë“œ
            chunks_path = os.path.join(index_dir, 'chunks.pkl')
            with open(chunks_path, 'rb') as f:
                self.chunks = pickle.load(f)
            
            print(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.chunks)}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


def test_rag_system():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # PDF ì²˜ë¦¬
    processor = PDFProcessor()
    chunks = processor.process_pdf("test.pdf")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = RAGSystemLite()
    rag.build_index(chunks)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_queries = [
        "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì¬ì„¤ì •í•˜ëŠ” ë°©ë²•ì€?",
        "ì˜ì—…ì‹œê°„ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì—°ë½ì²˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
    ]
    
    for query in test_queries:
        print(f"\nì§ˆë¬¸: {query}")
        result = rag.generate_answer(query)
        
        if result['answer']:
            print(f"ë‹µë³€: {result['answer'][:200]}...")
            print(f"ì‹ ë¢°ë„: {result['confidence']}")
            print(f"ìœ ì‚¬ë„: {result['similarity']:.3f}")
            print(f"ì¶œì²˜: í˜ì´ì§€ {result['sources'][0]['page']}")
        else:
            print("ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    test_rag_system()
