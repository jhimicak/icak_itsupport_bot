"""
RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ
- ì„ë² ë”© ìƒì„± (sentence-transformers)
- ë²¡í„° ê²€ìƒ‰ (FAISS)
- ë‹µë³€ ìƒì„±
"""

import os
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict, Tuple
from pdf_processor import PDFProcessor


class RAGSystem:
    """RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):
        """
        Args:
            model_name: sentence-transformers ëª¨ë¸ ì´ë¦„ (í•œêµ­ì–´ ì§€ì›)
        """
        print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        
        # FAISS ì¸ë±ìŠ¤
        self.index = None
        self.chunks = []
        
        print(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {self.dimension})")
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë°°ì—´ (n_texts, dimension)
        """
        embeddings = self.model.encode(texts, show_progress_bar=True)
        return np.array(embeddings).astype('float32')
    
    def build_index(self, chunks: List[Dict[str, any]]):
        """
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° FAISS ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            chunks: PDF ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not chunks:
            print("âš ï¸ ì²­í¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ”„ {len(chunks)}ê°œ ì²­í¬ ì¸ë±ì‹± ì¤‘...")
        
        # ì²­í¬ ì €ì¥
        self.chunks = chunks
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = [chunk['text'] for chunk in chunks]
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.create_embeddings(texts)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„± (L2 ê±°ë¦¬ ê¸°ë°˜)
        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(embeddings)
        
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")
    
    def search(self, query: str, top_k: int = 3) -> List[Tuple[Dict, float]]:
        """
        ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            (ì²­í¬, ê±°ë¦¬) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if self.index is None or self.index.ntotal == 0:
            print("âš ï¸ ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return []
        
        # ì§ˆë¬¸ ì„ë² ë”©
        query_embedding = self.create_embeddings([query])
        
        # ê²€ìƒ‰
        distances, indices = self.index.search(query_embedding, min(top_k, self.index.ntotal))
        
        # ê²°ê³¼ ì •ë¦¬
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(dist)))
        
        return results
    
    def generate_answer(self, query: str, top_k: int = 3, distance_threshold: float = 1.5) -> Dict:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            distance_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
            
        Returns:
            ë‹µë³€ ë”•ì…”ë„ˆë¦¬
        """
        # ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
        results = self.search(query, top_k)
        
        if not results:
            return {
                'answer': None,
                'sources': [],
                'confidence': 'none'
            }
        
        # ê°€ì¥ ìœ ì‚¬í•œ ê²°ê³¼ í™•ì¸
        best_chunk, best_distance = results[0]
        
        # ì„ê³„ê°’ ì²´í¬
        if best_distance > distance_threshold:
            return {
                'answer': None,
                'sources': [],
                'confidence': 'low',
                'distance': best_distance
            }
        
        # ë‹µë³€ ìƒì„± (ìƒìœ„ ê²°ê³¼ ì¡°í•©)
        answer_parts = []
        sources = []
        
        for chunk, distance in results[:2]:  # ìƒìœ„ 2ê°œë§Œ ì‚¬ìš©
            if distance <= distance_threshold:
                answer_parts.append(chunk['text'])
                sources.append({
                    'page': chunk['metadata'].get('page_number', '?'),
                    'source': chunk['metadata'].get('source', ''),
                    'distance': distance
                })
        
        if not answer_parts:
            return {
                'answer': None,
                'sources': [],
                'confidence': 'low'
            }
        
        # ë‹µë³€ í¬ë§·íŒ…
        answer = "\n\n".join(answer_parts)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = 'high' if best_distance < 0.8 else 'medium'
        
        return {
            'answer': answer,
            'sources': sources,
            'confidence': confidence,
            'distance': best_distance
        }
    
    def save_index(self, index_dir: str):
        """
        ì¸ë±ìŠ¤ì™€ ì²­í¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            index_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        """
        os.makedirs(index_dir, exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        index_path = os.path.join(index_dir, 'faiss.index')
        faiss.write_index(self.index, index_path)
        
        # ì²­í¬ ì €ì¥
        chunks_path = os.path.join(index_dir, 'chunks.pkl')
        with open(chunks_path, 'wb') as f:
            pickle.dump(self.chunks, f)
        
        print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_dir}")
    
    def load_index(self, index_dir: str) -> bool:
        """
        ì €ì¥ëœ ì¸ë±ìŠ¤ì™€ ì²­í¬ ë¡œë“œ
        
        Args:
            index_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            index_path = os.path.join(index_dir, 'faiss.index')
            self.index = faiss.read_index(index_path)
            
            # ì²­í¬ ë¡œë“œ
            chunks_path = os.path.join(index_dir, 'chunks.pkl')
            with open(chunks_path, 'rb') as f:
                self.chunks = pickle.load(f)
            
            print(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")
            return True
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


def test_rag_system():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # PDF ì²˜ë¦¬
    processor = PDFProcessor()
    chunks = processor.process_pdf("test.pdf")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = RAGSystem()
    rag.build_index(chunks)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_queries = [
        "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì¬ì„¤ì •í•˜ëŠ” ë°©ë²•ì€?",
        "ì˜ì—…ì‹œê°„ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì—°ë½ì²˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
    ]
    
    for query in test_queries:
        print(f"\nì§ˆë¬¸: {query}")
        result = rag.generate_answer(query)
        
        if result['answer']:
            print(f"ë‹µë³€: {result['answer'][:200]}...")
            print(f"ì‹ ë¢°ë„: {result['confidence']}")
            print(f"ì¶œì²˜: í˜ì´ì§€ {result['sources'][0]['page']}")
        else:
            print("ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    test_rag_system()
